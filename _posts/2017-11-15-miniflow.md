---
layout: post
title:  "MiniFlow"
date:   2017-11-15 00:00:00 +0900
categories: dlnd
fbcomments: true
---

Outline:

- [What is a Neural Network](#what-is-a-neural-network)
- [Graphs](#graphs)
- [Gradient Descent](#gradient-descent)

## What is a Neural Network

- A neural network is **a graph of mathematical functions**:
    - **linear combinations** and **activation functions**. 
- The graph consists of **nodes**, and **edges**.

![example-neural-network]({{ "/assets/img/miniflow/example-neural-network.png" | absolute_url }}){: .center-image }{:height="400px"}

- **Nodes** in each layer (except for nodes in the input layer) **perform mathematical functions** using inputs from nodes in the previous layers. 
    - For example, a node could represent $$f(x,y)=x+y$$, where $$x$$ and $$y$$ are **input values from nodes in the previous layer**.
- Each **node creates an output value** which may be passed to nodes in the next layer. 
    - The output value from the output layer does not get passed to a future layer (last layer!)
- Layers between the input layer and the output layer are called **hidden layers**.
- The **edges** in the graph describe **the connections between the nodes**, along which the values flow from one layer to the next. 
    - These edges can also apply operations to the values that flow along them, such as **multiplying by weights, adding biases**, etc..

## Graphs

- The **nodes** and **edges** create a **graph structure**. 
- It isn't hard to imagine that increasingly **complex graphs** can calculate **almost anything**.
- There are generally **two steps to create neural networks**:
    - **Define the graph** of nodes and edges.
    - **Propagate values** through the graph.

## Gradient Descent

- Technically, the gradient actually **points uphill**, in the direction of **steepest ascent**. 
- But if we put a `-` sign in front of this value, we get the direction of **steepest descent**, which is what we want.
- **How much force** should be applied to the **push**. This is known as the **learning rate**.
    - which is an apt name since this value determines how quickly or slowly the neural network learns.
- This is more of a guessing game than anything else but empirically values in the range **0.1 to 0.0001 work well**. 
    - The range **0.001 to 0.0001 is popular**, as **0.1 and 0.01 are sometimes too large**.

**Gradient-Descent-Convergence:**
![gradient-descent-convergence]({{ "/assets/img/miniflow/gradient-descent-convergence.gif" | absolute_url }}){: .center-image }{:height="200px"}

**Gradient-Descent-Civergence:**
![gradient-descent-divergence]({{ "/assets/img/miniflow/gradient-descent-divergence.gif" | absolute_url }}){: .center-image }{:height="200px"}


Code:
```python
"""
Given the starting point of any `x` gradient descent
should be able to find the minimum value of x for the
cost function `f` defined below.
"""
import random
from gd import gradient_descent_update


def f(x):
    """
    Quadratic function.

    It's easy to see the minimum value of the function
    is 5 when is x=0.
    """
    return x**2 + 5


def df(x):
    """
    Derivative of `f` with respect to `x`.
    """
    return 2*x

def gradient_descent_update(x, gradx, learning_rate):
    """
    Performs a gradient descent update.
    """
    # TODO: Implement gradient descent.
    
    # Return the new value for x
    return x - learning_rate * gradx

# Random number between 0 and 10,000. Feel free to set x whatever you like.
x = random.randint(0, 10000)
# TODO: Set the learning rate
learning_rate = 0.1
epochs = 100

for i in range(epochs+1):
    cost = f(x)
    gradx = df(x)
    print("EPOCH {}: Cost = {:.3f}, x = {:.3f}".format(i, cost, gradx))
    x = gradient_descent_update(x, gradx, learning_rate)

```

Output:
```bash
EPOCH 0: Cost = 2601774.000, x = 3226.000
EPOCH 1: Cost = 1665137.160, x = 2580.800
EPOCH 2: Cost = 1065689.582, x = 2064.640
EPOCH 3: Cost = 682043.133, x = 1651.712
EPOCH 4: Cost = 436509.405, x = 1321.370
EPOCH 5: Cost = 279367.819, x = 1057.096
EPOCH 6: Cost = 178797.204, x = 845.677
EPOCH 7: Cost = 114432.011, x = 676.541
EPOCH 8: Cost = 73238.287, x = 541.233
EPOCH 9: Cost = 46874.304, x = 432.986
EPOCH 10: Cost = 30001.354, x = 346.389
EPOCH 11: Cost = 19202.667, x = 277.111
EPOCH 12: Cost = 12291.507, x = 221.689
EPOCH 13: Cost = 7868.364, x = 177.351
EPOCH 14: Cost = 5037.553, x = 141.881
EPOCH 15: Cost = 3225.834, x = 113.505
EPOCH 16: Cost = 2066.334, x = 90.804
EPOCH 17: Cost = 1324.254, x = 72.643
EPOCH 18: Cost = 849.322, x = 58.114
EPOCH 19: Cost = 545.366, x = 46.492
EPOCH 20: Cost = 350.834, x = 37.193
EPOCH 21: Cost = 226.334, x = 29.755
EPOCH 22: Cost = 146.654, x = 23.804
EPOCH 23: Cost = 95.658, x = 19.043
EPOCH 24: Cost = 63.021, x = 15.234
EPOCH 25: Cost = 42.134, x = 12.187
EPOCH 26: Cost = 28.766, x = 9.750
EPOCH 27: Cost = 20.210, x = 7.800
EPOCH 28: Cost = 14.734, x = 6.240
EPOCH 29: Cost = 11.230, x = 4.992
EPOCH 30: Cost = 8.987, x = 3.994
EPOCH 31: Cost = 7.552, x = 3.195
EPOCH 32: Cost = 6.633, x = 2.556
EPOCH 33: Cost = 6.045, x = 2.045
EPOCH 34: Cost = 5.669, x = 1.636
EPOCH 35: Cost = 5.428, x = 1.309
EPOCH 36: Cost = 5.274, x = 1.047
EPOCH 37: Cost = 5.175, x = 0.838
EPOCH 38: Cost = 5.112, x = 0.670
EPOCH 39: Cost = 5.072, x = 0.536
EPOCH 40: Cost = 5.046, x = 0.429
```